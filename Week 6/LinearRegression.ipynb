{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lineare Regression\n",
    "\n",
    "## Wat is het?\n",
    "\n",
    "Lineare regression is een Machine Learning techniek dat zoekt naar het verband tussen een set features en een target of output waarde.\n",
    "Omdat voor de training van het model de output-waarde gebruikt wordt van elk voorbeeld/training example voor het bepalen van het beste model (minimaliseren kostenfunctie) valt deze technieke binnen het gebied van supervised learning. \n",
    "\n",
    "In deze notebook wordt deze technieke uitgelegd en gedemonstreerd door middel van een voorbeeld dataset die we hieronder gaan genereren.\n",
    "In het begin gaan we werken met 1 feature maar in de praktijk is er geen limiet op hoeveel features er kunnen zijn. \n",
    "Het algoritme dat we hier gaan gebruiken gaat blijven werken maar meer features zorgen er wel voor dat er meer data/ examples beschikbaar moeten zijn omdat het verband moeilijker te vinden is. \n",
    "Dit zorgt er dan ook voor dat het veel langer kan duren voor het model te trainen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# graphical\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from ipywidgets import interact, FloatSlider\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "# sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo-data\n",
    "\n",
    "Eerst gaan we een demo-dataset aanmaken waarop we gaan lineare regressie toepassen. Dit kan door middel van onderstaande functie die 5 parameters heeft:\n",
    "* a = de richtingscoefficient van de gewenste data\n",
    "* b = de bias / verticale verschuiving van de gewenste data\n",
    "* xmin = de minimum waarde op de x-as\n",
    "* xmax = de maximum waarde op de x-as\n",
    "* noise = de maximum afwijking van elk datapunt ten opzichte van de rechte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_single_feature(a, b, xmin, xmax, N, noise):\n",
    "    x = np.random.uniform(low=xmin, high=xmax, size=(N,))\n",
    "    y = [a*i+b + np.random.uniform(low = -noise, high=noise) for i in x] # make data from equation and add noise\n",
    "    \n",
    "    return pd.DataFrame({\"feature\": x, \"output\":y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = generate_data_single_feature(10, 50, 0, 20, 100, 30)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot(kind='scatter',x='feature',y='output',grid=True,s=50,color='red', alpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wat is het beste model?\n",
    "\n",
    "Lineare Regressie gaat nu op zoek naar de beste lijn/curve/... die de beste match is voor dit model.\n",
    "Voor deze dataset zijn we op zoek naar een rechte dus de vergelijking voor deze rechte kunnen we schrijven als:\n",
    "\n",
    "$ f_{\\pmb{w}}(x) = w_0 + w_1 x$ = **target**\n",
    "\n",
    "In deze vergelijingen worden $w_0$ en $w_1$ de gewichten of parameters genoemd van het model.\n",
    "Deze kunnen ook als vector $\\pmb{w}$ voorgesteld worden.\n",
    "Regressie gaat dan op zoek naar de optimale waarden van deze gewichten.\n",
    "Dit gebeurt door **training** of **learning** van het model.\n",
    "\n",
    "Hoe bepaal je echter welke lijn het beste is? Welk van onderstaande lijnen is de beste match?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot(kind='scatter',x='feature',y='output',grid=True,s=50,color='red', alpha=0.4)\n",
    "plt.plot([0, 20], [60, 240], color='blue', lw=2)\n",
    "plt.plot([0, 20], [30, 260], color='green', lw=2)\n",
    "plt.plot([0, 20], [50, 250], color='orange', lw=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om te bepalen welke rechte de beste is voor deze dataset moeten we gebruik maken van een kostenfunctie.\n",
    "Deze functie berekent voor elke combinatie van gewichten een bepaalde kost.\n",
    "Het optimaliseren van de gewichten komt dan overeen met het minimaliseren van de kost functie / het zoeken van de gewichten met een minimale kost.\n",
    "\n",
    "Een veelgebruikte kostfunctie $L(\\pmb{w})$ bij Lineare Regressie is Least Mean Squares (of het gemiddelde van de fout). De vergelijking van deze kostfunctie is:\n",
    "\n",
    "$ L(\\pmb{w}) = \\frac{1}{2N}\\sum\\limits_{i=1}^{N} (f_{\\pmb{w}}(x^i) - y^i)^2$\n",
    "\n",
    "Hier wordt $L$ gebruikt als notatie voor de kostenfunctie omdat deze ook vaak Lossfunction genoemd wordt.\n",
    "In deze vergelijking stelt $\\pmb{w}$ de gewichten voor, $x^i$ en $y^i$ respectievelijk de input en output van het $i$-de trainingsexample en $N$ het totaal aantal trainings examples.\n",
    "Merk op dat het belangrijk is om het gemiddelde van de totale fout te gebruiken (delen door $N$) omdat anders de fout afhankelijk is van het totaal aantal trainingsexamples.\n",
    "Een tweede opmerking is dat de extra deling door 2 enkel wiskundig belangrijk is omdat hier later de afgeleide moet van genomen worden. \n",
    "Al deze wiskunde moet je niet kunnen uitvoeren of programmeren.\n",
    "Dit gebeurd achter de schermen door de verschillende libraries die we gaan gebruiken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = data.sample(10)\n",
    "s.sample(10).plot(kind='scatter',x='feature',y='output',grid=True,s=50,color='red', alpha=0.4)\n",
    "plt.plot([0, 20], [0, 200], color='blue', lw=2)\n",
    "\n",
    "def draw_error(row):\n",
    "    plt.plot([row.feature, row.feature], [row.output, row.feature*10], color=\"purple\")\n",
    "\n",
    "s.apply(draw_error, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bij het zoeken naar de beste rechte kunnen we twee gewichten aanpassen om de kostenfunctie te minimaliseren, namelijk de richtingscoefficient en de verschuiving. \n",
    "Met de onderstaande code kunnen deze gewichten manueel veranderd worden.\n",
    "Zoek nu eens zelf naar de optimale waarden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LMS(x,y,w0, w1):\n",
    "    pred = x*w1+w0\n",
    "    return ((y - pred)**2).sum() / 2 / len(x)  \n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "#fig.tight_layout()\n",
    "\n",
    "def setWeights(w0,w1):\n",
    "    lms_w1 = LMS(data.feature, data.output, w0, w1)\n",
    "    lms_xAxis = np.linspace(-20,50,100)\n",
    "    lms_yAxis = np.array([LMS(data.feature, data.output, w0, w1tmp) for w1tmp in lms_xAxis])\n",
    "    \n",
    "    ax1 = axes[0]\n",
    "    ax1.clear()\n",
    "    ax1.plot(w1, lms_w1,'ro')\n",
    "    ax1.plot(lms_xAxis, lms_yAxis )\n",
    "    #ax.legend(loc=2); # legende linksboven\n",
    "    ax1.set_xlabel('w1')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('LMS - cost');\n",
    "    \n",
    "    ax2 = axes[1]\n",
    "    ax2.clear()\n",
    "    xAxis = np.array([data.feature.min(), data.feature.max()])\n",
    "    yAxis = w0 + xAxis * w1\n",
    "    ax2.scatter(data.feature, data.output)\n",
    "    ax2.plot(xAxis,yAxis,'r')\n",
    "    ax2.set_ylim(0,300)\n",
    "    ax2.set_xlabel('feature')\n",
    "    ax2.set_ylabel('output')\n",
    "    \n",
    "    #print('Error = ', lms_w1)\n",
    "    fig.canvas.draw()\n",
    "    \n",
    "    \n",
    "interact(setWeights,w0=FloatSlider(min=-100, max=100.0, step=1, value=0),w1=FloatSlider(min=-20, max=50, step=1, value=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dit is eigenlijk net hetzelfde wat excel doet als er een trendlijn bepaalt wordt door de punten. Echter Excel kan dit enkel doen bij eenvoudige verbanden en loopt vast als het om complexere datasets gaat.\n",
    "\n",
    "Het voorgaande toonde enkel 1 gewicht op de x-as omdat het een 2D-plot was.\n",
    "Met behulp van een 3D-plot kan de totale kostenfunctie getoond worden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0_x = np.linspace(-100,100,100)\n",
    "w1_x = np.linspace(-20,20,100)\n",
    "lms_y = np.array([\n",
    "    [LMS(data.feature, data.output, w0, w1) for w1 in w1_x]\n",
    "    for w0 in w0_x\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x ,y = np.meshgrid(w0_x, w1_x)\n",
    "\n",
    "fig3D = plt.figure()\n",
    "axis3D = fig3D.add_subplot(111, projection='3d')\n",
    "axis3D.plot_surface(x, y, lms_y, rstride=7, cstride=7,cmap=cm.coolwarm)\n",
    "axis3D.set_xlabel('w0')\n",
    "axis3D.set_ylabel('w1')\n",
    "axis3D.set_title('Kostenfunctie LMS');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Hoe vinden van de optimale gewichten\n",
    "\n",
    "Hoe kunnen nu met behulp van deze kostenfunctie, de laagste kost gevonden worden?\n",
    "Algebraisch uitrekenen met behulp van matrices is een mogelijkheid tot als je werkt met heel veel features.\n",
    "Een andere mogelijkheid is door gebruik te maken van gradient descent.\n",
    "Dit is het grote achterliggende idee van heel veel verschillende Machine Learning technieken dus is het belangrijk om te weten hoe dit werkt.\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "In plaats van onmiddelijk de meest optimale waarde uit te rekenen zoals bij de algebraische methode, wordt er in deze manier stapje per stapje geoptimaliseerd op basis van de kostenfunctie.\n",
    "\n",
    "Waar is bijvoorbeeld het laagste punt op onderstaande foto? Hoe zou je ernaar toe gaan?\n",
    "![dal](images/dal.jpg)\n",
    "\n",
    "Je weet dat je op ergens op de berg begint maar je weet niet exact waar je naartoe moet. \n",
    "Je kijkt dus rond je en wandelt in de richting die het snelst naar beneden gaat. \n",
    "Dit is eigenlijk net wat Gradient Descent doet. \n",
    "Er wordt begonnen op een willekeurige plaats en afhankelijk van de helling (afleiding) van de kostenfunctie worden de gewichten aangepast.\n",
    "Deze aanpassing gebeurt door gebruik te maken van de onderstaande formules\n",
    "\n",
    "$ \\frac{dL(\\pmb{w})}{dw_0} = \\frac{1}{N}\\sum\\limits_{i=1}^{N} (w_1x^i + w_0 - y^i)$\n",
    "\n",
    "$ \\frac{dL(\\pmb{w})}{dw_1} = \\frac{1}{N}\\sum\\limits_{i=1}^{N} (w_1x^i + w_0 - y^i)x^i$\n",
    "\n",
    "$w_0 = w_0 - \\eta \\frac{dL(\\pmb{w})}{dw_0}$\n",
    "\n",
    "$w_1 = w_1 - \\eta \\frac{dL(\\pmb{w})}{dw_1}$\n",
    "\n",
    "waarin $\\eta$ de learning rate voorstelt.\n",
    "\n",
    "**Kan je door deze manier te gebruiken vast komen te zitten in een lokaal minimum?**\n",
    "\n",
    "De meeste klassieke technieken in Machine Learning hebben geen last van lokale minima en vinden onmiddelijk het globale minimum.\n",
    "Dit komt omdat de kostenfunctie gebruikt in deze algoritmes convex is. \n",
    "Er zijn dus geen plotselinge stijgingen halverwege de daling wat voor lokale minima kan zorgen.\n",
    "\n",
    "#### Learning rate\n",
    "\n",
    "Dat bepaalt hoe snel je evolueert naar het optimum.\n",
    "Een goede keuze voor deze parameter is belangrijk:\n",
    "* te klein, dan duurt het zeer lang voor je het optimum bereikt\n",
    "* te groot, dan kan het zijn dat je steeds over het minimum springt waardoor het ook lang kan duren.\n",
    "* in het begin groot en later traag of een variabele learning rate zorgt voor een goede combinatie van de twee.\n",
    "\n",
    "Door veel van de algoritmes wordt de learning rate zelf aangepast zodat als ze merken dat je overshoot of dat je het optimum benaderd dat de learning rate verlaagd wordt.\n",
    "\n",
    "![learning rate](images/learningrate.png)\n",
    "\n",
    "#### Training van het model door middel van bestaande libaries\n",
    "\n",
    "Natuurlijk zou je bovenstaande formules zelf kunnen implementeren in python om zou je model te trainen, maar gelukkig is dit niet nodig omdat dit algoritme reeds geimplementeerd is door de libary [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).\n",
    "Het trainen van de verschillende technieken in deze library volgt grotendeels hetzelfde stramien. \n",
    "Eerst moet het gewenste algoritme opgezet worden en eventueel een aantal parameters gekozen worden. \n",
    "Daarna wordt het model getrained met een **fit** methode.\n",
    "Een getrained model kan je daarna gebruiken om voorspellingen te doen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lineaire regressie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Enkelvoudige naar meervoudige regressie\n",
    "\n",
    "In de praktijk zullen er bitter weinig situaties zijn waar een variabele voorspeld kan en moet worden door 1 feature.\n",
    "* Kwaliteit van wijn hangt af van zuurtegraad, suikergehalte, dichtheid, alcohol percentage, bewaringstemperatuur\n",
    "* Warmteverlies van het huis op basis van aantal vensters, type glas, muurisolatie, dakisolatie, ...\n",
    "\n",
    "Gelukkig is het bovenstaande eenvoudig uit te breiden naar meerdere features.\n",
    "Per extra feature gaat er nog 1 extra gewicht bij komen.\n",
    "\n",
    "**Waarom moeten er geen twee gewichten bijkomen?**\n",
    "\n",
    "Dit komt omdat de gewichten die de verschuiving bijhouden kunnen samengevoegd worden tot 1 gewicht.\n",
    "\n",
    "De formule voor $f_{\\pmb{w}}(x)$ ziet er dan als volgt uit voor het geval met $n$ features:\n",
    "\n",
    "$f_{\\pmb{w}} = w_0 + w_1 x_1 + w_2x_2 + \\dots + w_n x_n$\n",
    "\n",
    "Het eindresultaat is dus een lineaire combinatie van alle features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_multi_feature(coeffs, b, mins, maxs, N, noise):\n",
    "    result = pd.DataFrame()\n",
    "    result[\"y\"] = np.random.uniform(low = -noise, high=noise, size=(N,)) + b\n",
    "    for i, coeff in enumerate(coeffs):\n",
    "        col = \"x_\" + str(i)\n",
    "        result[col] = np.random.uniform(low=mins[i], high=maxs[i], size=(N,))\n",
    "        result[\"y\"] =result[\"y\"] + result[col] * coeff\n",
    "    \n",
    "    return result\n",
    "\n",
    "data_3 = generate_data_multi_feature([10,5,-9], 30, [-10, -5, 0], [0, 8, 17], 1000, 10)\n",
    "data_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lineaire regressie voor meerdere features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Splitsen van features en targets\n",
    "\n",
    "Het target mag niet mee gebruikt worden voor als feature, anders gaat je model geen voorspellingen kunnen maken want er gaat geleerd worden om gewoon je target terug te geven.\n",
    "Dit afsplitsen kan gebeuren door je de kolom met de targets eens te droppen en eens te selecteren, bijvoorbeeld:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset splitsen in training en test data\n",
    "\n",
    "Wanneer je je model traint op je volledige dataset, dan is het moeilijk om je model te evalueren. Er zijn dan twee opties:\n",
    "* Evalueren op een sample van je data maar die zijn reeds gezien dus is niet realistisch.\n",
    "* Nieuwe data verzamelen voor je model te testen maar dit kan kostelijk zijn.\n",
    "\n",
    "Daarom is het beter om je dataset in twee te splitsen (later zelfs in drie):\n",
    "* Een trainingsset om je model te trainen (vaak een 70-80%)\n",
    "* Een testset om je model te evalueren (een 20-30%)\n",
    "\n",
    "Indien je over weinig data beschikt kan het aangeraden zijn om je testset kleiner te maken.\n",
    "De eenvoudigste manier om dit te splitsen is door gebruik te maken van de [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function.\n",
    "Er zijn twee manieren om de grootte van de testset te bepalen. \n",
    "Indien test_size tussen 0 en 1 is, wordt het beschouwd als het percentage van de dataset dat test-data moet zijn. \n",
    "Indien het een geheel getal groter dan 1 is, dan worden dat aantal observaties gekozen als testdata. \n",
    "Daarnaast kan ook het shuffle-argument gezet worden. \n",
    "Dit gaat je dataset eerst doorheen shufflen zodat de volgorde niet meer dezelfde is. \n",
    "Dit is zeker aangeraden om te doen in het geval dat er gesorteerd kan zijn in je dataset.\n",
    "Enkel in het geval van tijdreeksen kan dit voor problemen zorgen om dat daar net de volgorde belangrijk is. \n",
    "Let er ook op dat je met het argument random_state de random number generator kan instellen voor het shuffelen reproduceerbaar te maken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitsen test en trainingsdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluatie van het model\n",
    "\n",
    "Gewoon kijken naar de resulterende coefficienten kan wel een indicatie geven of het model goed getrained is maar een gevoel bij de correcte werking is geen bewijs dus moet de accuraatheid va het model berekend worden.\n",
    "Hiervoor zijn er een aantal maatstafen die berekend kunnen worden.\n",
    "\n",
    "### Mean Absolute error\n",
    "\n",
    "De Mean absolute error is het gemiddelde van de absolute waarden van het verschil tussen de voorspelde waarden $\\hat{y}_i$ en de werkelijke waarden $y_i$.\n",
    "Deze wordt berekend als volgt:\n",
    "\n",
    "$MAE = \\frac{1}{N} \\sum\\limits_{i=1}^{N}|y_i-\\hat{y}_i|$\n",
    "\n",
    "Het is mogelijk om deze zelf te berekenen aan de hand van bovenstaande functie maar er is ook een functie in sklearn die dit reeds doet, namelijk mean_absolute_error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared error\n",
    "\n",
    "Een andere mogelijkheid is om het gemiddelde te nemen van het kwadraad van de fout in plaats van het maximum. \n",
    "Deze methode leunt dicht aan bij de kostfunctie de we minimaliseren en ook bij de variantie van de verdeling. \n",
    "Hierdoor wordt deze metriek ook vaak gebruikt om de variantie van de fout in te schatten.\n",
    "Merk op dat door het kwadraten een groter gewicht toegekend wordt aan grote fouten.\n",
    "De vergelijking om deze metriek te bereken is: \n",
    "\n",
    "$MSE = \\frac{1}{N} \\sum\\limits_{i=1}^{N}(y_i-\\hat{y}_i)^2$\n",
    "\n",
    "Net zoals bij MAE is er een functie in sklearn om deze waarde te berekenen, namelijk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rsquared\n",
    "\n",
    "Een derde mogelijkheid die veel gebruikt wordt is de determinatiecoefficient $R^2$.\n",
    "Deze wordt berekend als quotient van de varianties van de voorspelde waarden en de werkelijke waarden van van de target.\n",
    "Deze geeft weer hoeveel van de variabiliteit in de output verklaard wordt door het model.\n",
    "Deze waarde is een getal tussen 0 en 1 en kan geinterpreteerd worden als een percentage.\n",
    "In het geval van $R^2=1$ zijn de voorspellingen perfect. \n",
    "\n",
    "De code om deze maatstaf te berekenen is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "In veel gevallen is er nog optimalisatie mogelijk in de features van de dataset.\n",
    "Deze optimaliseren of nieuwe bedenken valt onder feature engineering.\n",
    "Twee belangrijke technieken in het geval van regularisatie zijn normalisatie en hogere-orde features\n",
    "\n",
    "### Normalisation\n",
    "\n",
    "In deze techniek wordt ervoor gezorgd dat de features die gebruikt worden bij de regressie op dezelfde schaalverdeling staan.\n",
    "Dit is ten eerste belangrijk om ervoor te zorgen dat de gebruikte gewichten ook in dezelfde schaal vallen (wat belangrijk gaat zijn later bij regularisatie).\n",
    "Daarnaast kan het ook een grote impact hebben op het aantal stappen dat nodig is om het optimale resultaat te bereiken. \n",
    "Dit komt omdat de grootte van de stappen gezet richting het optimum afhankelijk is van de kleinste stap.\n",
    "\n",
    "Normalisatie op een feature kan doorgevoerd worden door gebruik te maken van de standardScaler.\n",
    "Deze gaat per feature (dus elke kolom afzonderlijk) het gemiddelde en standaard afwijking berekenen om de schaal dan aan te passen zodat deze genormaliseerd zijn. \n",
    "Dat wil dus zeggen gemiddelde 0 en standaardafwijking 1.\n",
    "De code hiervoor is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierbij is het belangrijk om op te merken dat je de scaler niet opnieuw moet fitten voor de testdata. \n",
    "Deze moet herschaald worden door gebruik te maken van dezelfde scaler als voor de trainingsdata.\n",
    "\n",
    "Dit is 1 mogelijke scaler, andere manieren zijn:\n",
    "* MaxAbsScaler: Deelt elk getal door het maximum in die kolom\n",
    "* MinMaxScaler: Trek van elk getal het minimum in die kolom af en deel het door het verschil tussen het maximum en minimum.\n",
    "* In het geval van beelden kan je alles delen door 255 (de maximum waarde van een pixel)\n",
    "\n",
    "### Higher-order features\n",
    "\n",
    "In bovenstaande modellen hebben we steeds lineare verbanden gebruikt.\n",
    "Dit is echter vaak niet het geval. \n",
    "Hoe zou je bijvoorbeeld een rechte teken door de punten op onderstaande scatterplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_square_feature(a, b, xmin, xmax, N, noise):\n",
    "    x = np.random.uniform(low=xmin, high=xmax, size=(N,))\n",
    "    y = [a*i**2+b + np.random.uniform(low = -noise, high=noise) for i in x]\n",
    "    \n",
    "    return pd.DataFrame({\"feature\": x, \"output\":y})\n",
    "\n",
    "data_square = generate_data_square_feature(10, 10, 0, 100, 100, 1000)\n",
    "display(data_square.head())\n",
    "data_square.plot(kind='scatter',x='feature',y='output',grid=True,s=50,color='red', alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lineaire regressie voor higher order features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dit komt omdat ons huidige model te eenvoudig is en niet in staat is om de kwadratisch verband te modelleren.\n",
    "Een oplossing hiervoor is om extra features te gaan toevoegen van een hogere orde, bijvoorbeeld een tweede feature die het kwadraat is van de eerste feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_square[\"x2\"] = data_square.feature**2\n",
    "\n",
    "data_square.plot(kind='scatter',x='feature',y='output',grid=True,s=50,color='red', alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lineaire regressie met kwadratische features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hetzelfde kan ook gedaan worden door features van nog hogere orde te introduceren (bijvoorbeeld derde of vierde graad).\n",
    "Wel belangrijk is om erop te letten wat er gebeurt net buiten het bereik van je data, bijvoorbeeld in dit geval gaat het voor negatieve getallen terug beginnen stijgen wat niet altijd het verwachte resultaat is.\n",
    "\n",
    "Er bestaan ook geautomatiseerde methoden om deze features van hogere orde toe te voegen.\n",
    "Door gebruik te maken van de PolynomialFeatures functionaliteit in sklearn kan men alle features van een willekeurige graad toevoegen.\n",
    "Let op, dat hierdoor ook alle combinaties ook toegevoegd worden dus voor de features \"a\" en \"b\" en voor een gewenste graad 3 komen deze features uiteindelijk in de dataset terecht: $a, a^2, a^3, b, ab, a^2b, b^2, ab^2, b^3$.\n",
    "Het is dus duidelijk dat het aantal features zeer snel kan stijgen dus wees voorzichtig met de gekozen graad.\n",
    "Onderstaande code is een voorbeeld van deze automatische manier om features toe te voegen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatisch hogere orde features toevoegen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra features\n",
    "\n",
    "Buiten de standaard features kan je ook extra features bedenken, zoals:\n",
    "* De oppervlakte van het huis op basis van de lengte en breedte\n",
    "* Uit start- en eindpunt de afstand halen\n",
    "* Uit een datum, de dag van de week of de maand halen.\n",
    "* Gemiddelde helderheid van een beeld\n",
    "* Contouren detecteren in een beeld\n",
    "\n",
    "Elke van deze nieuwe features kan meer informatie geven aan het model om extra verbanden te zoeken.\n",
    "\n",
    "## Underfitting\n",
    "\n",
    "Underfitting treedt op wanneer het model de beschikbare training data niet kan modelleren en dus ook niet op de testdata.\n",
    "Dit komt vaak omdat er ofwel te weinig data beschikbaar is voor het model of dat het model te eenvoudig is (denk aan het model zonder hogere-orde features hierboven).\n",
    "\n",
    "## Overfitting\n",
    "\n",
    "Dit is het tegenovergestelde probleem.\n",
    "Het model leert in dit geval te veel over de trainingsinformatie waardoor de fout op de trainingsset zeer klein is maar de fout op de testset significant groter is dan die op de trainingsset.\n",
    "Men kan zeggen dat het model in dit geval te complex is voor de beschikbare data waardoor ruis in de data ook geleerd wordt. \n",
    "Vooral Neurale Netwerken zijn hier heel gevoelig aan. \n",
    "Onderstaande toont een voorbeeld van een te complex model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_over = generate_data_single_feature(10, 10, 0, 10, 4, 10)\n",
    "data_over.plot(kind='scatter',x='feature',y='output',grid=True,s=50,color='red', alpha=0.4)\n",
    "\n",
    "graad = 10\n",
    "poly = PolynomialFeatures(graad)\n",
    "poly.fit(pd.DataFrame(data_over.feature))\n",
    "X_train_over = poly.transform(pd.DataFrame(data_over.feature))\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_over, data_over.output)\n",
    "\n",
    "x_predicts = np.linspace(1,10,200)\n",
    "x_predicts = poly.transform(x_predicts.reshape(-1, 1))\n",
    "predicts = model.predict(x_predicts)\n",
    "\n",
    "plt.plot(x_predicts[:,1], predicts)\n",
    "plt.ylim(bottom=0, top=200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Er zijn twee manieren om dit op te lossen:\n",
    "* Meer data lost meestal dit probleem wel op (maak hierboven eens ipv 4 samples 1000 samples aan als trainingsdata en de resultaten zullen helemaal anders zijn)\n",
    "* Regularisatie\n",
    "\n",
    "## Regularisatie\n",
    "\n",
    "Methode om overfitting tegen te gaan en een goed evenwicht te zoeken tussen overfitting en underfitting.\n",
    "Het werkt door een component toe te voegen aan de kostenfunctie die de regularisatieterm genoemd wordt.\n",
    "De bedoeling van deze extra term is een kost toe te kennen aan het gebruik van hoge coefficienten zodat grote fluctuaties afgeraden worden tenzij ze een sterke daling van de kostenfunctie als resultaat hebben.\n",
    "De uiteindelijke uitdrukking van de kostenfunctie is dan:\n",
    "\n",
    "$ L(\\pmb{w}) = \\frac{1}{2N}\\sum\\limits_{i=1}^{N} (f_{\\pmb{w}}(x^i) - y^i)^2 + \\lambda R(\\pmb{w})$\n",
    "\n",
    "De $\\lambda$ laat toe om de mate van regularisatie te tunen. \n",
    "Deze parameter (net zoals de graad van de hogere ordes) wordt een hyperparameter genoemd.\n",
    "Een goede inschatting van deze hyperparameters kan een grote invloed hebben op het uiteindelijke resultaat van het model.\n",
    "In python wordt de $\\lambda$ parameter de $\\alpha$ parameter genoemd maar de betekenis is net hetzelfde.\n",
    "\n",
    "**L2-norm**\n",
    "\n",
    "Hierbij is de regularisatieterm de som van het kwadraat van de gewichten. \n",
    "De code om deze vorm van regularisatie te gebruiken is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regressie met l2-regularisatie\n",
    "\n",
    "data_over.plot(kind='scatter',x='feature',y='output',grid=True,s=50,color='red', alpha=0.4)\n",
    "plt.plot(x_predicts[:,1], predicts)\n",
    "plt.ylim(bottom=0, top=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Het is duidelijk dat door het toevoegen van het model, de sterke fluctuaties eruit zijn gehaald en het resultaat een heel stuk beter is. \n",
    "Merk ook op dat we hier onze features nog hebben genormalizeerd door de parameter mee te geven bij het aanmaken van het model.\n",
    "\n",
    "**L1-Norm**\n",
    "\n",
    "Een andere veel gebruikte regularisatieterm is de L1-Norm. \n",
    "Deze wordt berekend door het gemiddelde te nemen van de absolute waarden van de gewichten.\n",
    "Het voordeel van deze term is dat er gewichten volledig op nul kunnen gezet worden indien ze niet bijdragen tot het voorspellen van de target.\n",
    "De code om met deze term aan de slag te gaan is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regressie met l1-regularisatie\n",
    "\n",
    "data_over.plot(kind='scatter',x='feature',y='output',grid=True,s=50,color='red', alpha=0.4)\n",
    "plt.plot(x_predicts[:,1], predicts)\n",
    "plt.ylim(bottom=0, top=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als afsluiter nog een tip om na te gaan of er under- of overfitting aanwezig is in het getrainde model.\n",
    "Laat de target voor je trainingsdata voorspellen.\n",
    "* Als zowel deze score als de testscore laag is, dan is er underfitting.\n",
    "* Als de score van de trainingsdata hoog is maar die van de testdata laag, dan is er overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R^2 = \\frac{\\sum\\limits_{i=1}^{N}(\\hat{y}_i-\\bar{y})^2}{\\sum\\limits_{i=1}^{N}(y_i-\\bar{y})^2}$\n",
    "\n",
    "## Oefeningen:\n",
    "\n",
    "Om de bovenstaande technieken verder in te oefenen kan de huispruis voorspeld worden op basis van een aantal omgevingsfactoren met behulp van [deze dataset](https://www.kaggle.com/vikrishnan/boston-house-prices). \n",
    "Let er wel op dat de delimiter in deze dataset een spatie is in plaats van een komma dus dit gaat nog moeten meegegeven worden bij het inladen van het bestand.\n",
    "Probeer minstens de volgende zaken uit:\n",
    "* Maak een correlatiematrix aan en teken deze om de feature die de hoogste correlatie bevat te vinden.\n",
    "* Maak voor deze correlatiematrix ook een [pairplot](https://seaborn.pydata.org/generated/seaborn.pairplot.html). Dit geeft voor elke combinatie van features een scatter plot die gebruikt kan worden om het verband tussen de verschillende features in te schatten.\n",
    "* Probeer enkelvoudige lineare regressie uit door de huisprijs te voorspellen op basis van 1 feature. Wat is de hoogste score die je bekomt?\n",
    "* Probeer meervoudige regressie uit om alle features te gebruiken? Wat is de impact op de score?\n",
    "* Voeg de 2-orde feature toe voor de gekozen feature van de lineare regressie. Voer de regressie opnieuw uit en bestudeer de impact op de score van het model. Verklaar ook de impact.\n",
    "* Zoek nu naar het beste model dat je kan vinden. Gebruik hiervoor alle features, hogere-orde features en regressie. Wat is de beste score die je bekomt en wat waren de gekozen hyperparameters hiervoor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "d5e8e3a19af5ceb2434683dff87da6345c3b29f7eb0a8a138558c07d014a01cc"
   }
  },
  "widgets": {
   "state": {
    "0c6cc137e1c842ad8a1339d4909b50ed": {
     "views": [
      {
       "cell_index": 5
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
